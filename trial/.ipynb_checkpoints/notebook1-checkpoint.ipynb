{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8eac032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\deepv\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.0.1)\n",
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.7-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n",
      "   ---------------------------------------- 0.0/235.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 235.5/235.5 kB 4.8 MB/s eta 0:00:00\n",
      "Installing collected packages: fuzzywuzzy, unidecode\n",
      "Successfully installed fuzzywuzzy-0.18.0 unidecode-1.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2 fuzzywuzzy unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56d512b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import os\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d39229e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            if page_num > 0:  # Add newline for page breaks (except the first page)\n",
    "                text += \"\\n\"\n",
    "            text += pdf_reader.pages[page_num].extract_text()\n",
    "    text = unidecode(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebcf6c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_headings(text):\n",
    "    headings = []\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Define synonyms and their corresponding fuzz ratio thresholds\n",
    "    heading_synonyms = {\n",
    "        \"Education\": [\"Education\", \"Qualifications\", \"Educational Qualifications\", \"Academic Background\", \"Educational Details\", \"Education and Training\"],\n",
    "        \"Skills\": [\"Skills\", \"Technical Skills\", \"Key Competencies\", \"Skill Highlights\", \"Primary Skills\", \"Specializations\", \"Areas of Expertise\", \"Expertise\", \"Programming Languages\"],\n",
    "        \"Experience\": [\"Work Experience\", \"Professional Background\", \"Professional Experience\", \"Work History\", \"Teaching Experience\", \"Employment History\"],\n",
    "        \"Achievements\": [\"Accomplishments\", \"Achievements\", \"Notable Projects\", \"Qualifications\"],\n",
    "        \"Others\": [\"Awards\", \"Honors\", \"Recognition\", \"Publications\", \"Certifications\", \"Presentations\", \"Volunteer Experience\", \"Leadership Experience\",\"Interests\",\"Hobbies\", \"Languages\", \"Licenses\"],\n",
    "        \"Summary\": [\"Career Overview\", \"Summary\", \"About Me\", \"Profile Summary\", \"Highlights\", \"Objective\"]\n",
    "    }\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        for heading, synonyms in heading_synonyms.items():\n",
    "            for synonym in synonyms:\n",
    "                if re.match(r'^\\s*{}\\s*'.format(synonym), line, re.IGNORECASE):\n",
    "                    headings.append((line, heading))\n",
    "\n",
    "    found = False\n",
    "    for tup in headings:\n",
    "      if tup[1] == \"Experience\":\n",
    "          found = True\n",
    "\n",
    "    if not found:\n",
    "      for line in lines:\n",
    "        line = line.strip()\n",
    "        for heading, synonyms in heading_synonyms.items():\n",
    "          if heading == \"Experience\":\n",
    "            synonyms = [\"Work Experience\", \"Professional Background\", \"Professional Experience\", \"Work History\", \"Teaching Experience\", \"Employment History\", \"Experience\"]\n",
    "            for synonym in synonyms:\n",
    "                if re.match(r'^\\s*{}\\s*'.format(synonym), line, re.IGNORECASE):\n",
    "                    headings.append((line, heading))\n",
    "    return headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4657d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "    # Remove newline characters\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Remove tab characters\n",
    "    text = text.replace('\\t', ' ')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "897f718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_resume(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    headings = identify_headings(text)\n",
    "\n",
    "    resume_parts = {}\n",
    "    current_heading = \"\"\n",
    "    current_part = \"\"\n",
    "    capturing_work_experience = False\n",
    "\n",
    "    for line in text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if (line, \"Education\") in headings:\n",
    "            current_heading = \"Education\"\n",
    "            current_part = \"\"\n",
    "            capturing_work_experience = False\n",
    "        elif (line, \"Skills\") in headings:\n",
    "            current_heading = \"Skills\"\n",
    "            current_part = \"\"\n",
    "            capturing_work_experience = False\n",
    "        elif (line, \"Experience\") in headings:\n",
    "            current_heading = \"Experience\"\n",
    "            current_part = \"\"\n",
    "            capturing_work_experience = True\n",
    "        elif (line, \"Achievements\") in headings:\n",
    "            current_heading = \"Achievements\"\n",
    "            current_part = \"\"\n",
    "            capturing_work_experience = False\n",
    "        elif (line, \"Others\") in headings:\n",
    "            current_heading = \"Others\"\n",
    "            current_part = \"\"\n",
    "            capturing_work_experience = False\n",
    "        elif (line, \"Summary\") in headings:\n",
    "            current_heading = \"Summary\"\n",
    "            current_part = \"\"\n",
    "            capturing_work_experience = False\n",
    "        else:\n",
    "            if capturing_work_experience:\n",
    "                current_part += line + \"\\n\"\n",
    "                resume_parts.setdefault(current_heading, \"\")  # Initialize the dictionary key if not present\n",
    "                resume_parts[current_heading] += line + \"\\n\"\n",
    "            else:\n",
    "               if current_heading:\n",
    "                current_part += line + \"\\n\"\n",
    "                resume_parts[current_heading] = current_part\n",
    "\n",
    "    # Create a DataFrame from the parsed resume parts\n",
    "    fixed_columns = {\n",
    "        'ResumeID': '',\n",
    "        'Category':'',\n",
    "        'Education': '',\n",
    "        'Skills': '',\n",
    "        'Experience': '',\n",
    "        'Achievements': '',\n",
    "        'Others':'',\n",
    "        'Summary':''\n",
    "    }\n",
    "\n",
    "    # Update the fixed_columns dictionary with data from the input dictionary\n",
    "    fixed_columns.update(resume_parts)\n",
    "\n",
    "    # Create a DataFrame from the updated dictionary\n",
    "    df = pd.DataFrame([fixed_columns])\n",
    "    df = df.applymap(clean_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8dd853",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "root_dir = 'uploads/resumes'\n",
    "for root, dirs, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            # Extract text from the PDF file\n",
    "            pdf_path = os.path.join(root, file)\n",
    "            parsed_resume_df = parse_resume(pdf_path)\n",
    "            parsed_resume_df[\"ResumeID\"] = file.replace(\".pdf\",\"\")\n",
    "            parsed_resume_df[\"Category\"] = os.path.basename(root)\n",
    "            dfs.append(parsed_resume_df)\n",
    "dfs = pd.concat(dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69c1132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.to_csv(\"Resume_extracted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9d3d526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [51 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.6/44.6 kB ? eta 0:00:00\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU datasets transformers sentence-transformers git+https://github.com/naver/splade.git\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d18b84",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6d7716d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'splade'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mast\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msplade\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer_rep\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Splade\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m      7\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'splade'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "from splade.models.transformer_rep import Splade\n",
    "from transformers import AutoTokenizer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df844a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
